Metadata-Version: 2.1
Name: evf_sam2
Version: 1.0
Summary: EVF SAM: Efficient Video Frame Segmentation with Segment Anything Model
Home-page: https://github.com/facebookresearch/segment-anything-2
Author: Meta AI
Author-email: segment-anything@meta.com
License: Apache 2.0
Requires-Python: >=3.10.0
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: packaging
Requires-Dist: sentencepiece
Requires-Dist: markdown2
Requires-Dist: openai
Requires-Dist: ray
Requires-Dist: Requests
Requires-Dist: shortuuid
Requires-Dist: tqdm
Requires-Dist: scipy
Requires-Dist: bitsandbytes
Requires-Dist: blobfile
Requires-Dist: mypy
Requires-Dist: pytest
Requires-Dist: requests
Requires-Dist: tensorboardX
Requires-Dist: ftfy
Requires-Dist: pyarrow
Requires-Dist: torchmetrics==0.7.3
Requires-Dist: deepspeed
Requires-Dist: pycocoevalcap
Requires-Dist: torchscale==0.2.0
Requires-Dist: hydra-core

<div align ="center">
<img src="assets/logo.jpg" width="20%">
<h1> ðŸ“· EVF-SAM </h1>
<h3> Early Vision-Language Fusion for Text-Prompted Segment Anything Model </h3>

[Yuxuan Zhang](https://github.com/CoderZhangYx)<sup>1,\*</sup>, [Tianheng Cheng](https://scholar.google.com/citations?user=PH8rJHYAAAAJ&hl=zh-CN)<sup>1,\*</sup>, Lei Liu<sup>2</sup>, Heng Liu<sup>2</sup>, Longjin Ran<sup>2</sup>, Xiaoxin Chen<sup>2</sup>, [Wenyu Liu](http://eic.hust.edu.cn/professor/liuwenyu)<sup>1</sup>, [Xinggang Wang](https://xwcv.github.io/)<sup>1,ðŸ“§</sup>

<sup>1</sup> Huazhong University of Science and Technology, <sup>2</sup> vivo AI Lab

(\* equal contribution, ðŸ“§ corresponding author)

[![arxiv paper](https://img.shields.io/badge/arXiv-Paper-red)](https://arxiv.org/abs/2406.20076)
[![ðŸ¤— HuggingFace models](https://img.shields.io/badge/HuggingFaceðŸ¤—-Models-orange)](https://huggingface.co/YxZhang/)  
[![ðŸ¤— HuggingFace Demo](https://img.shields.io/badge/HuggingFaceðŸ¤—-Demo-orange)](https://huggingface.co/spaces/wondervictor/evf-sam)*(evf-sam)*
[![ðŸ¤— HuggingFace Demo](https://img.shields.io/badge/HuggingFaceðŸ¤—-Demo-orange)](https://huggingface.co/spaces/wondervictor/evf-sam2)*(evf-sam2)*

</div>

## News
We have expanded our EVF-SAM to powerful [SAM-2](https://github.com/facebookresearch/segment-anything-2). Besides improvements on image prediction, our new model also performs well on video prediction (powered by SAM-2). Only at the expense of a simple image training process on RES datasets, we find our EVF-SAM has zero-shot video text-prompted capability. Try our code!

## Highlight
<div align ="center">
<img src="assets/architecture.jpg">
</div>

* EVF-SAM extends SAM's capabilities with text-prompted segmentation, achieving high accuracy in Referring Expression Segmentation.  
* EVF-SAM is designed for efficient computation, enabling rapid inference in few seconds per image on a T4 GPU.


## Updates
- [x] Release code
- [x] Release weights
- [x] Release demo ðŸ‘‰ [ðŸ¤— evf-sam](https://huggingface.co/spaces/wondervictor/evf-sam)
- [x] Release code and weights based on SAM-2
- [x] Update demo supporting SAM-2ðŸ‘‰ [ðŸ¤— evf-sam2](https://huggingface.co/spaces/wondervictor/evf-sam2)


## Visualization 
<table class="center">
<tr>
  <td style="text-align:center;"><b>Input text</b></td>
  <td style="text-align:center;"><b>Input image</b></td>
  <td style="text-align:center;"><b>Output</b></td>
</tr>
<tr>
  <td width=20% style="text-align:center;"><b>"zebra top left"</b></td>
  <td><img src="assets/zebra.jpg"></td>
  <td><img src="assets/zebra_vis.png"></td>
</tr> 

<tr>
  <td width=20% style="text-align:center;"><b>"a pizza with a yellow sign on top of it"</b></td>
  <td><img src="assets/pizza.jpg"></td>
  <td><img src="assets/pizza_vis.png"></td>
</tr> 

<tr>
  <td width=20% style="text-align:center;"><b>"the broccoli closest to the ketchup bottle"</b></td>
  <td><img src="assets/food.jpg"></td>
  <td><img src="assets/food_vis.png"></td>
</tr> 

<tr>
  <td width=20% style="text-align:center;"><b>"bus going to south common"</b></td>
  <td><img src="assets/bus.jpg"></td>
  <td><img src="assets/bus_vis.png"></td>
</tr> 

<tr>
  <td width=20% style="text-align:center;"><b>"3carrots in center with ice and greenn leaves"</b></td>
  <td><img src="assets/carrots.jpg"></td>
  <td><img src="assets/carrots_vis.png"></td>
</tr> 

</table>


## Installation
1. Clone this repository  
2. Install [pytorch](https://pytorch.org/) for your cuda version. **Note** that torch>=2.0.0 is needed if you are to use SAM-2, and torch>=2.2 is needed if you want to enable flash-attention. (We use torch==2.0.1 with CUDA 11.7 and it works fine.)
3. pip install -r requirements.txt
4. If you are to use the video prediction function, run:
```
cd model/segment_anything_2
python setup.py build_ext --inplace
```


## Weights
<table class="center">
<tr>
  <td style="text-align:center;"><b>Name</b></td>
  <td style="text-align:center;"><b>SAM</b></td>
  <td style="text-align:center;"><b>BEIT-3</b></td>
  <td style="text-align:center;"><b>Params</b></td>
  <td style="text-align:center;"><b>Prompt Encoder & Mask Decoder
  <td style="text-align:center;"><b>Reference Score</b></td>
</tr>

<tr>
  <td style="text-align:center;"><a href="https://huggingface.co/YxZhang/evf-sam2">EVF-SAM2</a></td>
  <td style="text-align:center;"><b>SAM-2-L</b></td>
  <td style="text-align:center;"><b>BEIT-3-L</b></td>
  <td style="text-align:center;"><b>898M</b></td>
  <td style="text-align:center;"><b>freeze</b></td>
  <td style="text-align:center;"><b>83.6</b></td>
</tr>

<tr>
  <td style="text-align:center;"><a href="https://huggingface.co/YxZhang/evf-sam">EVF-SAM</a></td>
  <td style="text-align:center;"><b>SAM-H</b></td>
  <td style="text-align:center;"><b>BEIT-3-L</b></td>
  <td style="text-align:center;"><b>1.32B</b></td>
  <td style="text-align:center;"><b>train</b></td>
  <td style="text-align:center;"><b>83.7</b></td>
</tr>

<tr>
  <td style="text-align:center;"><b>EVF-Effi-SAM-L </b></td>
  <td style="text-align:center;"><b>EfficientSAM-S</b></td>
  <td style="text-align:center;"><b>BEIT-3-L</b></td>
  <td style="text-align:center;"><b>700M</b></td>
  <td style="text-align:center;"><b>train</b></td>
  <td style="text-align:center;"><b>83.5</b></td>
</tr>

<tr>
  <td style="text-align:center;"><b>EVF-Effi-SAM-B </b></td>
  <td style="text-align:center;"><b>EfficientSAM-T</b></td>
  <td style="text-align:center;"><b>BEIT-3-B</b></td>
  <td style="text-align:center;"><b>232M</b></td>
  <td style="text-align:center;"><b>train</b></td>
  <td style="text-align:center;"><b>80.0</b></td>
</tr>
</table>

## Inference
### 1. image prediction
```
python inference.py  \
  --version <path to evf-sam> \
  --precision='fp16' \
  --vis_save_path "<path to your output direction>" \
  --model_type <"ori" or "effi" or "sam2", depending on your loaded ckpt>   \
  --image_path <path to your input image> \
  --prompt <customized text prompt>
```
`--load_in_8bit` and `--load_in_4bit` is **optional**  
for example: 
```
python inference.py  \
  --version YxZhang/evf-sam2 \
  --precision='fp16' \
  --vis_save_path "vis" \
  --model_type sam2   \
  --image_path "assets/zebra.jpg" \
  --prompt "zebra top left"
```

### 2. video prediction  
firstly slice video into frames
```
ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <frame_dir>/'%05d.jpg'
```
then:
```
python inference_video.py  \
  --version <path to evf-sam2> \
  --precision='fp16' \
  --vis_save_path "vis/" \
  --image_path <frame_dir>   \
  --prompt <customized text prompt>   \
  --model_type sam2
```
you can use frame2video.py to concat the predicted frames to a video.

## Demo
image demo
```
python demo.py <path to evf-sam>
```
video demo
```
python demo_video.py <path to evf-sam2>
```

## Data preparation
Referring segmentation datasets: [refCOCO](https://web.archive.org/web/20220413011718/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcoco.zip), [refCOCO+](https://web.archive.org/web/20220413011656/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcoco+.zip), [refCOCOg](https://web.archive.org/web/20220413012904/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refcocog.zip), [refCLEF](https://web.archive.org/web/20220413011817/https://bvisionweb1.cs.unc.edu/licheng/referit/data/refclef.zip) ([saiapr_tc-12](https://web.archive.org/web/20220515000000/http://bvisionweb1.cs.unc.edu/licheng/referit/data/images/saiapr_tc-12.zip)) and [COCO2014train](http://images.cocodataset.org/zips/train2014.zip)  
```
â”œâ”€â”€ dataset
â”‚Â Â  â”œâ”€â”€ refer_seg
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ images
â”‚Â Â  â”‚Â Â  |   â”œâ”€â”€ saiapr_tc-12 
â”‚Â Â  â”‚Â Â  |   â””â”€â”€ mscoco
â”‚Â Â  â”‚Â Â  |       â””â”€â”€ images
â”‚Â Â  â”‚Â Â  |           â””â”€â”€ train2014
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ refclef
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ refcoco
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ refcoco+
â”‚Â Â  â”‚Â Â  â””â”€â”€ refcocog
```

## Evaluation
```
torchrun --standalone --nproc_per_node <num_gpus> eval.py   \
    --version <path to evf-sam> \
    --dataset_dir <path to your data root>   \
    --val_dataset "refcoco|unc|val" \
    --model_type <"ori" or "effi" or "sam2", depending on your loaded ckpt>
```

## Acknowledgement
We borrow some codes from [LISA](https://github.com/dvlab-research/LISA/tree/main), [unilm](https://github.com/microsoft/unilm), [SAM](https://github.com/facebookresearch/segment-anything), [EfficientSAM](https://github.com/yformer/EfficientSAM), [SAM-2](https://github.com/facebookresearch/segment-anything-2).

## Citation
```bibtex
@article{zhang2024evfsamearlyvisionlanguagefusion,
      title={EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model}, 
      author={Yuxuan Zhang and Tianheng Cheng and Rui Hu and Lei Liu and Heng Liu and Longjin Ran and Xiaoxin Chen and Wenyu Liu and Xinggang Wang},
      year={2024},
      eprint={2406.20076},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.20076}, 
}
```
